[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference\n\n\n\n\n\nNotes for @ramirezSVAR2010\n\n\n\n\n\n\nDec 2, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\nBayesian VARs - The Likelihood Function\n\n\n\n\n\n(Bayesian) VARs form the backbone of modern macroeconometrics, but deriving the likelihood of a VAR in a form that is useful for Baysian calculations is not necessarily the most straighforward task. In this blog post I show how the VAR likelihood can be written in a form that is useful for Bayesian calculations.\n\n\n\n\n\n\nMay 7, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software/TinyGibbs/index.html",
    "href": "software/TinyGibbs/index.html",
    "title": "TinyGibbs.jl",
    "section": "",
    "text": "While learning more about VAR models, I noticed that many of the BayesianEstimation methods rely on Gibbs sampling. Although Julia has many Bayesian libraries, I could not find a Gibbs sampling library that I quite liked. What I wanted was a library that allows one to write a sampler just like one would dicsuss on in a paper. That is, I wanted a sampler that abstract away all the actual computational work and lets me focus on the actual conditional distributions. What I wanted was a sampler that lets me translate a statement like\n\nSample \\(\\alpha\\) from \\(p(\\alpha | y, x, \\Sigma)\\)\nSample \\(\\Sigma\\) from \\(p(\\Sigma | y, x, \\alpha)\\)\n\ninto a valid sampler.\nTinyGibbs is my answer to this need. TinyGibbs introduced a single macro, @tiny_gibbs which transforms a statement that is as close as possible to actual pseudo code in papers into a valid sampler. Additionally, by exploiting the functionality provided in AbstractMCMC and MCMCChains, TinyGibbs is able to sample in parallel and to diagnose the resulting MCMC chains.\nFor more information and to try it out, check out the GitHub repo. Any feedback is greatly appreciated."
  },
  {
    "objectID": "software/SnT/index.html",
    "href": "software/SnT/index.html",
    "title": "SnT_BigTime, SnT_VARS, and BigTime",
    "section": "",
    "text": "As part of my student assistantship to Ines Wilms, I got the chance to develop a set of interactive notebooks introducing VAR models and the BigTime library to which I got to contribute minor parts. These notebooks can be checked out on their respective GitHub repositories.\n\nIntroduction to VARs\nIntroduction to BigTime\nBigTime library"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi üëãüèª",
    "section": "",
    "text": "I‚Äôm Enrico. I am a PhD candiate at the School of Business and Economics Maastricht. I am interested in causal and Bayesian macro-econometrics."
  },
  {
    "objectID": "posts/var-likelihood/index.html",
    "href": "posts/var-likelihood/index.html",
    "title": "Bayesian VARs - The Likelihood Function",
    "section": "",
    "text": "\\[\n\\newcommand{\\bm}{\\mathbf}\n\\newcommand{\\bA}{\\bm A}\n\\newcommand{\\ba}{\\bm a}\n\\newcommand{\\bB}{\\bm B}\n\\newcommand{\\bb}{\\bm b}\n\\newcommand{\\bC}{\\bm C}\n\\newcommand{\\bc}{\\bm c}\n\\newcommand{\\bD}{\\bm D}\n\\newcommand{\\bd}{\\bm d}\n\\newcommand{\\bE}{\\bm E}\n\\newcommand{\\be}{\\bm e}\n\\newcommand{\\bF}{\\bm F}\n\\newcommand{\\bof}{\\bm f}\n\\newcommand{\\bG}{\\bm G}\n\\newcommand{\\bg}{\\bm g}\n\\newcommand{\\bH}{\\bm H}\n\\newcommand{\\bh}{\\bm h}\n\\newcommand{\\bI}{\\bm I}\n\\newcommand{\\bi}{\\bm i}\n\\newcommand{\\bJ}{\\bm J}\n\\newcommand{\\bj}{\\bm j}\n\\newcommand{\\bK}{\\bm K}\n\\newcommand{\\bk}{\\bm k}\n\\newcommand{\\bL}{\\bm L}\n\\newcommand{\\bl}{\\bm l}\n\\newcommand{\\bM}{\\bm M}\n\\newcommand{\\bom}{\\bm m}\n\\newcommand{\\bN}{\\bm N}\n\\newcommand{\\bn}{\\bm n}\n\\newcommand{\\bO}{\\bm O}\n\\newcommand{\\bo}{\\bm o}\n\\newcommand{\\bP}{\\bm P}\n\\newcommand{\\bp}{\\bm p}\n\\newcommand{\\bQ}{\\bm Q}\n\\newcommand{\\bq}{\\bm q}\n\\newcommand{\\bR}{\\bm R}\n\\newcommand{\\br}{\\bm r}\n\\newcommand{\\bS}{\\bm S}\n\\newcommand{\\bs}{\\bm s}\n\\newcommand{\\bT}{\\bm T}\n\\newcommand{\\bt}{\\bm t}\n\\newcommand{\\bU}{\\bm U}\n\\newcommand{\\bu}{\\bm u}\n\\newcommand{\\bV}{\\bm V}\n\\newcommand{\\bv}{\\bm v}\n\\newcommand{\\bW}{\\bm W}\n\\newcommand{\\bw}{\\bm w}\n\\newcommand{\\bX}{\\bm X}\n\\newcommand{\\bx}{\\bm x}\n\\newcommand{\\bY}{\\bm Y}\n\\newcommand{\\by}{\\bm y}\n\\newcommand{\\bZ}{\\bm Z}\n\\newcommand{\\bz}{\\bm z}\n% bold Greek lowercase letters for vectors\n\\newcommand{\\balpha}{\\bm \\alpha}\n\\newcommand{\\bbeta}{\\bm \\beta}\n\\newcommand{\\bgamma}{\\bm \\gamma}\n\\newcommand{\\bdelta}{\\bm \\delta}\n\\newcommand{\\bepsilon}{\\bm \\epsilon}\n\\newcommand{\\bvarepsilon}{\\bm \\varepsilon}\n\\newcommand{\\bzeta}{\\bm \\zeta}\n\\newcommand{\\boeta}{\\bm \\eta}\n\\newcommand{\\btheta}{\\bm \\theta}\n\\newcommand{\\biota}{\\bm \\iota}\n\\newcommand{\\bkappa}{\\bm \\kappa}\n\\newcommand{\\blambda}{\\bm \\lambda}\n\\newcommand{\\bmu}{\\bm \\mu}\n\\newcommand{\\bnu}{\\bm \\nu}\n\\newcommand{\\bxi}{\\bm \\xi}\n\\newcommand{\\bpi}{\\bm \\pi}\n\\newcommand{\\brho}{\\bm \\rho}\n\\newcommand{\\bsigma}{\\bm \\sigma}\n\\newcommand{\\btau}{\\bm \\tau}\n\\newcommand{\\bupsilon}{\\bm \\upsilon}\n\\newcommand{\\bphi}{\\bm \\phi}\n\\newcommand{\\bchi}{\\bm \\chi}\n\\newcommand{\\bpsi}{\\bm \\psi}\n\\newcommand{\\bomega}{\\bm \\omega}\n\\]\nA standard Baysian Vector AutoRegression (VAR) takes the form\n\\[\n\\mathbf{y}_t = C\\bar{\\mathbf{y}}_t + \\sum_{i=1}^p A_i \\mathbf{y}_{t-i} + \\mathbf{\\varepsilon}_t\n\\]\nwhere \\(\\mathbf{y}_t\\) is a \\(m \\times 1\\) vector of endogenous time series, \\(\\bar{\\mathbf{y}}_t\\) is a \\(m_c \\times 1\\) vector of constants and other deterministic trends, and \\(A_i\\) for \\(i=1, ..., p\\) and \\(C\\) are coefficient matrices. In total, there are \\(k\\equiv mp + m_c\\) coefficients in each equation, resulting in a total of \\(mk\\) coefficients in the model.\nWithout further specifying the distribution for the error terms, \\(\\mathbf{\\varepsilon}_t\\), no likelihood can be derived. The most common choice is to assume that the errors follow a multivariate Normal distribution with mean zero and covariance matrix \\(\\Sigma\\). That is, most commonly we assume that \\(\\mathbf{\\varepsilon}_t \\sim N(\\mathbf{0}, \\Sigma)\\). This then implies that\n\\[\n\\mathbf{y}_t | C, A_1, ..., A_p, \\Sigma, \\mathbf{y}_{t-1}, ..., \\mathbf{y}_{t-p} \\sim N(C\\bar{\\mathbf{y}}_t + \\sum_{i=1}^p A_i \\mathbf{y}_{t-i}, \\Sigma)\n\\]\nwhich is then used to derive the for of the likelihood that is classically used. For the Bayesian setting, it turns out, that there is a more convenient form of the likelihood. While both are functionally equivalent, the way Baysians write down the likelihood allows them to easily calculate the posterior given a prior on all parameters.\nTo derive the more convenient form, define \\(A = [c \\quad A_1 \\quad ... \\quad A_p]'\\) and \\(\\mathbf{x}_t = [\\bar{\\mathbf{y}}_t', \\mathbf{y}_{t-1}', ..., \\mathbf{y}_{t-p}']\\) where the former is of dimenions \\(k \\times m\\) and the latter is a row vector of dimensions \\(1\\times k\\). Using these definitions, we can write the VAR as\n\\[\n\\mathbf{y}_t = A'\\mathbf{x}_t' + \\mathbf{\\varepsilon}_t \\Rightarrow \\mathbf{y}_t' = \\mathbf{x}_t A + \\mathbf{\\varepsilon}_t'\n\\]\nThis form is convenient, because it allows us to define\n\\[\n\\begin{array}{ccc}\nY = \\begin{bmatrix}\\mathbf{y}_1' \\\\ \\vdots \\\\ \\mathbf{y}_T'\\end{bmatrix} &\nX = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\vdots \\\\ \\mathbf{x}_T\\end{bmatrix} &\nE = \\begin{bmatrix}\\mathbf{\\varepsilon}_1 \\\\ \\vdots \\\\ \\mathbf{\\varepsilon}_T\\end{bmatrix}\n\\end{array}\n\\]\nWe can then write the VAR in matrix notation as\n\\[\n\\DeclareMathOperator{\\vect}{vec}\n\\DeclareMathOperator{\\cov}{cov}\nY = XA + E \\Rightarrow \\mathbf{y} = (I_m \\otimes X)\\mathbf{\\alpha} + \\mathbf{\\varepsilon}\n\\]\nwhere \\(y = \\vect(Y)\\), \\(\\mathbf{\\alpha} = vec(A)\\) and \\(\\mathbf{\\varepsilon} = \\vect(E)\\). The vectorised form follows directly from the rules of the \\(\\vect\\) operator which can be found in e.g.¬†the matrix cookbook.\nWhat are the distributions of \\(E\\) and \\(\\mathbf{\\varepsilon}\\) though? I will pospone the discussion of the distribution of \\(E\\) to another post, since it is of Matrix Normal form and not many seem to know of the Matrix Normal distributions. Instead, I will focus on the distribution of \\(\\mathbf{\\varepsilon}\\) which is Multivariate Normal which is directly inhereted form the Multivarite Normal distribution of \\(\\mathbf{\\varepsilon}_t\\). To fully describe the distributions, we need its mean and covariance. The mean is easy; since every \\(\\varepsilon_{it}\\) has mean \\(\\mathbf{0}\\), we must have that \\(\\mathbf{\\varepsilon}\\) has mean \\(\\mathbf{0}\\). The covariance is a bit more tricky, and it helps to consider a small example case.\nLets suppose for a minute that \\(T=2\\) and \\(m=2\\). Then, \\(\\mathbf{\\varepsilon} = [\\varepsilon_{11}, \\varepsilon_{12}, \\varepsilon_{21}, \\varepsilon_{22}]'\\). Since errors are uncorrelated over time, we have in general that \\(\\cov(\\varepsilon_{it}, \\varepsilon_{jt'})=0\\) for all \\(i,j \\in \\{1, ...., m\\}\\) and all \\(t\\neq t'\\). We also know the covariance of any two errors at the same point in time: \\(\\cov(\\varepsilon_{it}, \\varepsilon_{jt})=\\sigma_{ij}=[\\Sigma]_{ij}\\) - the \\(ij\\) element of the covariance matrix \\(\\Sigma\\). Thus, in our small example, the covariance structure of \\(\\mathbf{\\varepsilon}\\) must take the form\n\\[\n\\begin{bmatrix}\n\\sigma_{11} & 0 & \\sigma_{12} & 0 \\\\\n0 & \\sigma_{11} & 0 & \\sigma_{12} \\\\\n\\sigma_{21} & 0 & \\sigma_{22} & 0 \\\\\n0 & \\sigma_{21} & 0 & \\sigma_{22}\n\\end{bmatrix} = \\Sigma \\otimes I_T\n\\]\nA careful look at the form above reveals that it is equal to \\(\\Sigma \\otimes I_T\\). This generally holds, implying that \\(\\mathbf{\\varepsilon} \\sim N(\\mathbf{0}, \\Sigma \\otimes I_T)\\).\n\n\n\n\n\n\nA small useful trick\n\n\n\n\n\nBefore we continue out derivation of the likelihood, it is useful to first derive a small algebraic trick. The general problem is, that we are given a quadratic form\n\\[\n(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta)\n\\]\nand we want to rewrite the above into a quadratic form in \\(\\bbeta\\) and \\(\\by\\). We thus first write\n\\[\n\\begin{split}\n&(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta) \\\\\n=& (\\by - X\\hat\\bbeta + X\\hat\\bbeta X\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta + X\\hat\\bbeta X\\bbeta) \\\\  \n=& (\\by - X\\hat\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta) + (\\bbeta - \\hat\\bbeta)X'\\Sigma^{-1}X(\\bbeta - \\hat\\bbeta) + 2(\\by - X\\hat\\bbeta)'\\Sigma^{-1}(X\\hat\\bbeta - X\\bbeta)\n\\end{split}\n\\]\nWe would like to get rid of the last term by choosing a smart \\(\\hat\\bbeta\\neq \\bbeta\\). For the last term to be zero, it is sufficient to find a \\(\\hat\\bbeta\\) that sets \\(\\by - X\\hat\\bbeta = 0\\).\n\\[\n\\begin{split}\n&\\by - X\\hat\\bbeta = 0 \\\\\n\\Leftrightarrow & \\by = X\\hat\\bbeta \\\\\n\\Leftrightarrow & X'\\by = X'X\\hat\\bbeta \\\\\n\\Rightarrow & \\hat\\bbeta = (X'X)^{-1}X'\\by\n\\end{split}\n\\]\nThus, by choosing \\(\\hat\\bbeta\\) to be the standard OLS estimate, we can rewrite the quadratic involving both \\(\\by\\) and \\(\\bbeta\\) into two quadratic forms, each involving only one of \\(\\by\\) and \\(\\bbeta\\). That is, for \\(\\hat\\bbeta = (X'X)^{-1}X'\\by\\) we have\n\\[\n(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta) = (\\by - X\\hat\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta) + (\\bbeta - \\hat\\bbeta)'X'\\Sigma^{-1}X(\\bbeta - \\hat\\bbeta)\n\\]\n\n\n\nSo we know that \\(\\by = (I_m \\otimes X)\\balpha + \\bvarepsilon\\) with \\(\\bvarepsilon \\sim N(\\bm 0, \\Sigma \\otimes I_T)\\). Thus, we also know \\(\\by \\sim N((I_m \\otimes X)\\balpha, \\Sigma \\otimes I_T)\\). Looking up the density of the multivariate Normal on, for example, wikipedia reveals that the likelihood can be proportially by written as\n\\[\n\\mathcal{L}(\\balpha, \\Sigma) \\propto |\\Sigma \\otimes I_T|^{-1/2}\\exp\\{-\\frac{1}{2}(\\by - (I_m \\times X)\\balpha)'(\\Sigma \\otimes I_T)^{-1}(\\by - (I_m \\otimes X)\\balpha)\\}\n\\]\nUsing the small trick above, we can rewrite this as\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma \\otimes I_T|^{-1/2}\\exp\\{-\\frac{1}{2}(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma \\times I_T)^{-1}(\\by - (I_m \\otimes X)\\hat\\balpha)\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(I_m\\otimes X)'(\\Sigma \\otimes I_T)(I_m \\otimes X)(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nwhere \\(\\hat\\balpha = (I_m \\otimes (X'X)^{-1}X')\\by\\). This can be massively simplified by making the following observations (see for example the matrix cookbook)\n\n\\(|A \\otimes B| = |A|^{rank(B)}|B|^{rank(A)}\\) and thus \\(|\\Sigma \\otimes I_T| = |\\Sigma|^T\\)\n\\((A \\otimes B)' = A' \\otimes B'\\) and thus \\((I_m \\otimes X)' = I_m \\otimes X'\\)\n\\((A \\otimes B)^{-1} = A^{-1}\\otimes B^{-1}\\) if the inverses exist. Thus \\((\\Sigma \\otimes I_T)^{-1} = \\Sigma^{-1}\\otimes I_T\\)\nIf all matrices are comformable, then \\((A\\otimes B)(C\\otimes D) = AD\\otimes BC\\) and thus\n\n\\[\n(I_m\\otimes X)'(\\Sigma\\otimes I_T)^{-1}(I_m \\otimes X) = (\\Sigma \\otimes (X'X)^{-1})^{-1}\n\\]\nUsing all of this in the likelihood, we can write\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-T/2}\\exp\\{\\frac{1}{2}(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nBefore we can come to the conclusion of this derivation, we need to rewrite one more part. For this, consider the first exponent term and write\n\\[\n\\begin{split}\n&(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m\\otimes X)\\hat\\balpha)\\\\\n= &[(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)]'[(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)]\n\\end{split}\n\\]\nCarefully looking at some of the kronecker product and \\(\\vect\\) operator properties, we can then recognise that\n\\[\n(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha) = \\vect((Y - X\\hat A)\\Sigma^{-1/2})\n\\]\nwhere \\(\\hat\\balpha = \\vect(\\hat A)\\). Thus, the term in the first exponent can be written as\n\\[\n\\begin{split}\n& (\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m\\otimes X)\\hat\\balpha) \\\\\n= &\\vect((Y - X\\hat A)\\Sigma^{-1/2})'\\vect((Y - X\\hat A)\\Sigma^{-1/2})\n\\end{split}\n\\]\nBy the properties of the trace, this equals\n\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\begin{split}\n\\vect((Y - X\\hat A)\\Sigma^{-1/2})'\\vect((Y - X\\hat A)\\Sigma^{-1/2}) &= \\tr((\\Sigma^{-1/2})'(Y-X\\hat A)'(Y-X\\hat A)\\Sigma^{-1/2}) \\\\\n&= \\tr(\\Sigma^{-1/2}(\\Sigma^{-1/2})'(Y - X\\hat A)'(Y-X \\hat A)) \\\\\n&= \\tr(\\Sigma^{-1}(Y - X\\hat A)'(Y - X\\hat A)) \\\\\n&= \\tr(\\underbrace{(Y-X\\hat A)'(Y - X\\hat A)}_{S}\\Sigma^{-1})\n\\end{split}\n\\]\nWe then finally arrive at the result that we were heading for\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-T/2}\\exp\\{\\frac{1}{2}\\tr(S\\Sigma^{-1})\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nWhy is this result so useful? Carefully looking at the expression above and comparing the forms to some known distributions, you might notice that the second exponential looks very much like a multivariate Normal distribution, while the first part looks very much like an Inverse-Wishart distribution. To actually get there, we still need to multiply and devide by \\(|\\Sigma \\otimes (X'X)^{-1}|^{-1/2}\\) which by the rules of the kronecker product is the same as \\(|\\Sigma|^{-k/2}|(X'X)^{-1}|^{-m/2}\\). Thus, after ignoring \\(|(X'X)^{-1}|^{-m/2}\\) because it does not depend on \\(\\balpha\\) or \\(\\Sigma\\), we can finally write the likelihood as\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-(T-k)/2}\\exp\\{\\frac{1}{2}\\tr(S\\Sigma^{-1})\\} \\\\\n&\\times |\\Sigma \\otimes (X'X)^{-1}|^{-1/2}\\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\} \\\\\n&=p(\\Sigma)p(\\balpha|\\Sigma)\n\\end{split}\n\\]\nIn the last like above I have already indicated that this is proportional to the product of a marginal distribution for \\(\\Sigma\\) and a conditional distribution for \\(\\balpha\\). From the functional forms we find that\n\\[\n\\begin{split}\n\\Sigma &\\sim IW(S, T-k-m-1) \\\\\n\\balpha | \\Sigma &\\sim N(\\hat\\balpha, \\Sigma \\otimes (X'X)^{-1})\n\\end{split}\n\\]\nSo again, why is this useful? Because it turns out that the likelihood is proportional to these two distributions. Why is that in turn useful? Because we know how to design smart priors for each part. That is, we know how to design a smart prior for the Inverse-Wishart part and we know how to design a smart prior for the conditional Normal part. With smart, I mean, among others, that we know conjugate priors for both parts. But even for non-conjugate priors, the form above is often more convenient to use than the classical form of the likelihood. I will show these benefits in future posts."
  },
  {
    "objectID": "posts/svar-identification/index.html",
    "href": "posts/svar-identification/index.html",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "",
    "text": "I first read Rubio-Ramirez, Waggoner, and Zha (2010) when I was taking a class on empirical macroeconomics during my masters. Although I already found the paper interesting back then, I cannot claim that I fully understood the importance of it; neither can I claim that I fully understood the theory. The latter I still cannot claim, but I think I have a much better understanding of its importance now. I would highly recommend it reading the paper. It will not be an easy read, and will probably require a few passes, but I think that the theory they provide is very intersting and should be part of every empirical macro class.\nBelow are some of my notes on Rubio-Ramirez, Waggoner, and Zha (2010). These notes are mostly written for myself, but I hope they can still be helpful for some others."
  },
  {
    "objectID": "posts/svar-identification/index.html#notes",
    "href": "posts/svar-identification/index.html#notes",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "Notes",
    "text": "Notes\nAlthough commonly used, the order condition stating that at least \\(n(n-1)/2\\) restrictions need to be imposed in order for a SVAR to be identified, is only necessary. Thus, even if a set of restriction satisfies this criterion, the model might still not be identified. Rubio-Ramirez, Waggoner, and Zha (2010) show this on an elegant example, but we can discuss it on an example with less realistic restrictions.\nSuppose that we are interested in identifying a SVAR with three variables: prices, real quantities, and the interest rate. We therefore would like to identify a supply, demand, and monetary policy shock. Various researchers might come up with various restrictions to impose. Suppose, three research groups came up with the following restrictions:\nFirst group:\n\nDue to sticky prices, a demand shock has no contemporaneous impact on prices. Additionally, due to a lag in official statistics, a demand shock has no contemporaneous impact on interest rates.\nAgain, due to sticky prices, a monetary policy shock has no contemporaneous impact on prices. However, we might be able to argue that because a lot of purchases are made by credit cards, and because interest on credit cards changes whenever the short-term rates change, monetary policy can have a contemporaneous impact on quantities.\nA supply shock is unrestricted.\n\nSecond group:\n\nDue to a lag in official statistics, a supply shock has no contemporaneous impact on interest rates. However, a supply shock does contemporaneously affect prices and quantities.\nAlso do to a lag in official statistics, a demand shock has no contemporaneous impact on interest rates. However, although prices are sticky, the observation period is long enough for some companies to be able to adjust prices. Thus, a demand shock has a contemporaneous impact on prices and on quantities.\nBy the same credit card argument as above, a monetary policy shock has a contemporaneous impact on quantities. However, it does not have a contemporaneous impact on prices, because firms‚Äô production costs are less likely to be highly dependent on interest rates in the short-run.\n\nThird group:\n\nDue to sticky prices, a monetary policy shock cannot contemporaneously affect prices. However, because a lot of spending is done using credit cards, monetary policy can have an immediate impact on quantities.\nBecause of a lag in official statistics, a supply shock has no immediate impact on interest rates. Additionally, the aggregate supply equation (determining the supply shock) is only a function of prices and quantities. Thus, the coefficient on interest rates is restricted to be zero.\nNo restrictions are imposed on a demand shock.\n\nThe first two research groups therefore impose restrictions only on the impact matrix, while the third group of researchers imposes some restrictions on the impact matrix, and additional restrictions on the contemporaneous coefficient matrix. However, counting the number of restrictions for all groups, we each time arrive at three. Thus, all three groups satisfy the order restrictions. The methodology developed in Rubio-Ramirez, Waggoner, and Zha (2010), however, can be used to show that only the SVAR of the first and third group is exactly identified.\nBefore discussing how the results in Rubio-Ramirez, Waggoner, and Zha (2010) can be used to show that the second SVAR is not exactly identified, it is helpful to first discuss what is meant with exact identification. Rubio-Ramirez, Waggoner, and Zha (2010) define exact identification as the case when for almost all reduced form parameter points, there exists a unique structural parameter point that satisfies the restrictions. Exact identification is therefore extremely desirable, because it states that no-matter what our reduced form estimates are, we will always be able to obtain a unique structural representation. Thus, we obtain identification results before we know what the reduced form estimates are.\nWhy is this not always the case? Following Rubio-Ramirez, Waggoner, and Zha (2010) we can parameterise a SVAR as \\[\ny_t'A_0 = \\sum_{l=1}^p y_{t-l}'A_l + \\varepsilon_t',\n\\tag{1}\\] where \\(y_t\\) is a \\(n\\times 1\\) vector of variables, and where the white noise vector \\(\\varepsilon_t\\) is a \\(n\\times 1\\) vector of structural shocks, such that \\(\\mathbb{E}[\\varepsilon_t]=\\mathbf{0}\\), \\(\\mathbb{E}[\\varepsilon_{i,t}\\varepsilon_{j,t}]=\\delta_{i,j}\\) with \\(\\delta_{i,j}\\) being the Kronecker delta. Writing, \\(A_+' = [A_1' \\dots A_p']\\), \\(x_t = [y_t', \\dots y_{t-p+1}']'\\), Equation¬†1 can be written as \\[\ny_t'A_0 = x_t'A_+ + \\varepsilon_t'.\n\\tag{2}\\] Rubio-Ramirez, Waggoner, and Zha (2010) call \\((A_0, A_+)\\) a structural parameter point.\nAssuming that \\(A_0\\) is invertible, Equation¬†2 can also be written as \\[\ny_t' = x_t'B_+ + u_t,\n\\tag{3}\\] where \\(B_+ = A_+A_0^{-1}\\) and \\(u_t' = \\varepsilon_tA_0^{-1}\\) with \\(\\mathbb{E}(u_tu_t') = (A_0A_0')^{-1} = \\Sigma\\). Rubio-Ramirez, Waggoner, and Zha (2010) call \\((B_+, \\Sigma)\\) a reduced form parameter point, and Equation¬†3 is called the reduced form system.\nNow, to understand why exact identification is such a unique result, we can have a look at a standard identification result in the SVAR literature. Consider two structural parameter points \\((A_0, A_+)\\) and \\((\\tilde A_0, \\tilde A_+)\\). If \\(\\tilde A_0 = A_0P\\) and \\(\\tilde A_+ = A_+P\\) with \\(P\\) being some invertible matrix, then \\(B_+ = A_+A_0^{-1} = A_+PP^{-1}A_0^{-1} = \\tilde A_+\\tilde A_0^{-1}\\). Thus, the two parameter points result in the same reduced-form coefficient matrix \\(B_+\\). If in addition \\(P^{-1}=P'\\), which is the case whenever \\(P\\) is a rotation matrix, then \\(\\Sigma = (A_0A_0')^{-1} = (A_0PP'A_0')^{-1} = (\\tilde A_0 \\tilde A_0')^{-1}\\), and thus also the reduced-form error-covariance matrix is the same. The two points \\((A_0, A_+)\\) and \\((\\tilde A_0, \\tilde A_+)\\) are then called observationally equivalent. Rubio-Ramirez, Waggoner, and Zha (2010) show that if two structural parameter points are observationally equivalent, then one is the rotation of another. If the two points are not just observationally equivalent but also both satisfy the imposed restrictions, then there exists no way to obtain a unique structural representation from the reduced form estimation. Exact identification is therefore the unique case in which the restrictions are just tight enough to rule out all except one structural representation for almost all reduced form points.\nExact identification is also related too global identification. A structural parameter point is globally identified if and only if there is no observationally equivalent other structural parameter point. Similarly, a structural parameter point is said to be locally identified if and only if there exists no other observationally equivalent structural parameter point within a neighbourhood. Exact identification is therefore global identification reduced to the subset of structural parameter points that satisfy the imposed restrictions.\nThe definition of exact identification requires a set of restrictions. This raises the question what these restrictions can be. Rubio-Ramirez, Waggoner, and Zha (2010) are rather general with regard to this. They allow for linear restrictions on \\(f(A_0, A_+)\\) where \\(f(A_0, A_+)\\) is a transformation with domain \\(U\\) that maps the structural parameter point \\((A_0, A_+)\\) to a set of \\(k \\times n\\) matrices where the columns correspond to the structural shocks in the system. Restrictions are then set on \\(f(A_0, A_+)\\) by requiring that \\[\nQ_jf(A_0, A_+)e_j = 0,\n\\] with \\(Q_j\\) being some \\(k \\times k\\) matrix representing linear restrictions and being of rank \\(q_j\\) (some rows might be all zero), and \\(e_j\\) being the \\(j\\)th unit vector. Although not necessary, a convenient ordering of the shocks is that that \\(q_1 \\geq q_2 \\geq \\dots \\geq q_n\\), such that the first shock has the most restrictions and the last shock has the least restrictions.\nExamples for \\(f(A_0, A_+)\\) include\n\n\\(f(A_0, A_+) = [A_0; A_+']\\): restrictions on the SVAR coefficient matrices.\n\\(f(A_0, A_+) = [\\Phi_i]\\) with \\(0\\leq i \\leq \\infty\\) and \\(\\Phi\\) being the IRF matrix (not infinite IRF is included).\nA combination of the two above.\n\nThe examples above are commonly restricted elements in the SVAR literature, with restriction on \\(\\Phi_0\\) being short-run restrictions, and restrictions on \\(\\Phi_\\infty\\) being long-run restrictions. One common property of all the restrictions above is that for any rotation matrix \\(P\\), it holds that \\(f(A_0P, A_+P) = f(A_0, A_+)P\\). Rubio-Ramirez, Waggoner, and Zha (2010) call this property admissible and require it to be true for any restrictions. For some results they also require regularity and strong regularity, both I will not define here, but they show that both hold for restrictions of the kind above.\nIn my opinion one of the most important results in Rubio-Ramirez, Waggoner, and Zha (2010) is that of Theorem 7.\n\n\n\n\n\n\nTheorem 7: Necessary and Sufficient Condition for Global Identification\n\n\n\nIn words: The SVAR is exactly identified if and only if the last shock has zero restrictions, the second to last has one restriction, etc. when the shocks are ordered in such a way that the last has the fewest restrictions, the second to last has the second to fewest restrictions etc. Conditions on restrictions are satisfied when putting restrictions on IRFs or structural matrices.\nStatement: Consider a SVAR with admissible and strongly regular restrictions represented by \\(R\\). The SVAR is exactly identified if and only if \\(q_j = n-j\\) for \\(1\\leq j \\leq n\\)\n\n\nTheorem 7 gives a condition that is as easy check as the original order condition. To see the power of this result, consider the examples from the beginning, where we put restrictions on \\(\\Phi_0\\), and \\(A_0\\).\nFirst group:\nFor the first research group, \\(f(A_0, A_+)=\\Phi_0\\) with variables ordered as \\(p_t, q_t, r_t\\), and shocks ordered as \\(\\varepsilon_t^s, \\varepsilon_t^d, \\varepsilon_t^r\\), the restriction matrix thus has the following pattern \\[\nf(A_0, A_+) =\n\\begin{bmatrix}\n* & 0 & 0 \\\\\n* & * & * \\\\\n* & 0 & *\n\\end{bmatrix},\n\\] where an asterisk denotes an unrestricted element. Since the demand shock has the most restrictions, the monetary policy shock the second most, and the supply shock the least, we can reorder the shocks in this order to obtain \\[\n\\begin{bmatrix}\n0 & 0 & * \\\\\n* & * & * \\\\\n0 & * & * \\\\\n\\end{bmatrix}.\n\\] There therefore exists two restrictions on the first column, one on the second, and zero on the last. By Theorem 7, this SVAR is exactly identified. In this special case, we could have also seen this by additionally ordering the variables as \\(q_t, r_t, p_t\\) resulting in\n\\[\n\\begin{bmatrix}\n* & * & * \\\\\n0 & * & * \\\\\n0 & 0 & * \\\\\n\\end{bmatrix}.\n\\] This is an upper-triangular system and can thus be found using a standard Cholesky identification scheme. However, as seen here, it sometimes is not immediately clear whether a system is triangular or not. The method of Rubio-Ramirez, Waggoner, and Zha (2010) is general enough to discover exact identification even if the research group does not realise that their system is triangular.\nSecond group:\nFor the second research group \\(f(A_0, A_+)=\\Phi_0\\), similar to the first group. However, different to the first group, the second group puts one restriction on each structural shock. Thus, \\(q_j = 1\\) for all \\(1 \\leq j \\leq 3\\). Theorem 7 can therefore never be satisfied, and thus the SVAR of the second research group is not exactly identified.\nThird group:\nFor the third research group \\(f(A_0, A_+)=[A_0; \\Phi_0]\\). With variables ordered as \\(q_t, p_t, r_t\\), and shocks ordered as \\(\\varepsilon_t^s, \\varepsilon_t^r, \\varepsilon_t^d\\), \\[\nf(A_0, A_+) =\n\\begin{bmatrix}\n* & * & * \\\\\n* & * & * \\\\\n0 & * & * \\\\\n* & * & * \\\\\n* & 0 & * \\\\\n0 & * & *\n\\end{bmatrix}.\n\\] Thus, there are two restrictions on the first shock, one restriction on the second shock, and zero restrictions on the last shock. Although restrictions are mixed over \\(A_0\\) and \\(\\Phi_0\\), Theorem 7 implies that this SVAR is still exactly identified.\nTheorem 7 gives an easy way to check whether restrictions are sufficient to achieve exact identification. However, Theorem 7 does not provide a way to obtain the unique structural parameter point from the reduced form estimates. To this end, Rubio-Ramirez, Waggoner, and Zha (2010) proof another useful result: Theorem 5.\n\n\n\n\n\n\nTheorem 5: Just rotate the Cholesky Identification\n\n\n\nIn words: If the SVAR is exactly identified, then the exactly identified structural parameter point is a rotation of some structural parameter point that does not satisfy all restrictions.\nStatement: Consider an SVAR with restrictions represented by \\(R\\). The SVAR is exactly identified if and only if, for almost every structural parameter point \\((A_0, A_+)\\in U\\), there exists a unique matrix \\(P\\in O(n)\\) such that \\((A_0P, A_+P)\\in R\\)\n\n\nTheorem 5 can be combined with Theorem 7 to obtain an efficient algorithm. First, use Theorem 7 to check whether the SVAR is exactly identified. If this is the case, then Theorem 5 implies that the unique structural parameter point satisfying the restrictions is a rotation of a parameter point that does not satisfy the restrictions. Finding a structural parameter point that potentially does not satisfy the restrictions is easy: Just use an arbitrary Cholesky scheme. Thus, if we could find the rotation matrix needed to satisfy the restrictions, then we can also find the unique structural parameter point."
  },
  {
    "objectID": "posts/svar-identification/index.html#the-algorithm",
    "href": "posts/svar-identification/index.html#the-algorithm",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "The Algorithm",
    "text": "The Algorithm\nHow do we find the rotation matrix of Theorem 5? The first step is to start with some arbitrary structural point \\((A_0, A_+)\\in U\\). For this point, there exists some \\(1 \\leq j \\leq n\\) such that \\[\nQ_jf(A_0, A_+)e_j \\neq 0.\n\\]\nRubio-Ramirez, Waggoner, and Zha (2010) propose the following algorithm to find the appropriate rotation matrix (here implemented in Julia).\n\nusing LinearAlgebra\nusing Random\n\n\"\"\"\nHelper function used to remove rows of zeros from Qj.\n\"\"\"\nfunction remove_zero_rows(Qj::Matrix)\n  non_zero_row = map(x -&gt; any(x .!= 0), eachrow(Qj))\n  return Qj[non_zero_row, :]\nend\n\n\"\"\"\nNormalise the vector to have unit length. \n\"\"\"\nmake_unit_length(x::Vector) = x ./ norm(x)\n\nnullspace_vec(Q::Matrix) = qr(Q').Q[:, end]\n\n\"\"\"\nFind the structural parameter point that satisfies the restrictions. \n\"\"\"\nfunction find_rotation_matrix(\n  A0::Matrix{T}, \n  Aplus::Matrix{T}, \n  f::Function,  # takes as arguments A0, Aplus and return f(A0, Aplus)\n  Qjs::Vector{Matrix{T}}, \n  normalise::Function  # takes as arguments A0, Aplus, P and return normalised P\n) where {T}\n\n  n = size(A0, 1)\n  P = similar(A0)\n  F = f(A0, Aplus)\n\n  # First iteration of step 2 of Algorithm 1\n  p1 = nullspace_vec(remove_zero_rows(Qjs[1])*F)\n  P[:, 1] = p1\n\n  # Remaining iterations in step 2 of Algorithm 2\n  for i=2:n\n    Q_tilde = vcat(remove_zero_rows(Qjs[i])*F, P[:, 1:i-1]')\n    pi = nullspace_vec(Q_tilde)\n    P[:, i] = pi\n  end\n\n  # Apply normalising rule\n  return normalise(A0, Aplus, P)\nend\n\nWe can test whether this algorithm works, by applying it to the third set of restrictions with the shocks in order of \\(\\varepsilon_t^s, \\varepsilon_t^r, \\varepsilon_t^d\\). The restrictions for the third research group are then given by\n\n# k = 6\nf(A0, Aplus) = [A0; inv(A0')]\n\nQjs = [zeros(6, 6) for _ in 1:3]\n# restrictions for supply shock \n# variables are ordered as q, r, p\nQjs[1][1, 2] = 1  # interest rate coefficient is zero \nQjs[1][2, 5] = 1  # no contemporaneous impact on interest rates\n# restrictions for monetary policy shock \nQjs[2][1, 6] = 1;  # no contemporaneous impact on prices\n\nNext to restrictions, we also need to impose a normalisation. A normalisation is needed, because if \\((A_0, A_+)\\) satisfies the restrictions, then so does \\((A_0D, A_+D)\\) with \\(D\\) being a diagonal matrix with only plus/minus one on the diagonal. To see this, note that \\(D\\) is a rotation matrix, and that if \\((A_0, A_+)\\) satisfies the restrictions, then \\[\nQ_jf(A_0D, A_+D) = Q_jf(A_0, A_+)De_j = \\pm Q_jf(A_0, A_+)e_j = 0 \\; \\forall 1 \\leq j \\leq n.\n\\] We therefore need to define a normalisation scheme. Rubio-Ramirez, Waggoner, and Zha (2010) define this formally. Since \\(PD\\) is another rotation matrix, we can simply normalise the rotation matrix that would solve the restrictions such that also the normalisation scheme is satisfied. The function below does exactly this and specifies our normalisation such that the supply shock has a positive impact on prices (a contractionary supply shock), the monetary policy shock has a positive impact on interest rates (monetary contraction), and the demand shock has a positive impact on quantities (expansionary demand shock).\n\nfunction normalise_P(A0_init, Aplus_init, P)\n  A0 = A0_init * P\n  Phi0 = inv(A0')\n  n = size(A0, 1)\n  D = diagm(ones(n))\n  for i = 1:n\n    if Phi0[i, i] &lt; 0\n      D[i, i] = -1\n    end\n  end\n  return P*D\nend\n\nGiven the restrictions and the normalisation, we are ready to test the algorithm on some artificial data. Here I will use a completely randomly created reduced-form error covariance matrix. Our initial guess for a structural point can then be as simple as the one obtained using a Cholesky identification scheme. Note that because of the parameterisation chosen by Rubio-Ramirez, Waggoner, and Zha (2010), the correct Cholesky part is now the upper-triangular part.\n\nRandom.seed!(1234)\nSigma = randn(3, 3)\nSigma = Symmetric(Sigma * Sigma')\n\nA0_init = inv(cholesky(Sigma).U)\nA0_init = Matrix(A0_init)\n@display A0_init\n\n3√ó3 Matrix{Float64}:\n 0.347663  -0.248945  -0.0710156\n 0.0        0.644327   0.115805\n 0.0        0.0        0.593772\n\n\nIt is very unlikely that this initial guess satisfies all the restrictions, but it is nonetheless good to check it. Unsurprisingly, it does not satisfy the restrictions.\n\nfunction is_satisfying_restrictions(A0, Aplus, f, Qjs)\n  F = f(A0, Aplus)\n  for j = 1:lastindex(Qjs)\n    all(isapprox.(Qjs[j]*F[:, j], 0; atol = sqrt(eps()))) || return false\n  end\n  return true\nend\n\n@show is_satisfying_restrictions(A0_init, A0_init, f, Qjs);\n\nis_satisfying_restrictions(A0_init, A0_init, f, Qjs) = false\n\n\nWe can then simply plug in the initial guess and the restrictions into the algorithm to obtain a rotation matrix. This rotation matrix can then be used to rotate the initial guess. The obtained point satisfies all restrictions, as promised by the algorithm.\n\nP = find_rotation_matrix(A0_init, A0_init, f, Qjs, normalise_P)\nA0 = A0_init * P\n\n@show is_satisfying_restrictions(A0, A0, f, Qjs);\n\nis_satisfying_restrictions(A0, A0, f, Qjs) = true\n\n\nEven though the new point satisfies all restrictions, it is still good to check the actual restriction matrix obtained. At the obtained parameter point, zeros do not just exist at the imposed positions in \\(f\\), but additional zeros were also introduced. In the identified SVAR, the interest rate coefficient is also zero for the equation specifying the demand shock. Additionally, a monetary policy shock has not just no contemporaneous impact on prices, but also not on quantities.\n\n@display round.(f(A0, A0); digits = 2)\n\n6√ó3 Matrix{Float64}:\n  0.06  -0.26  -0.34\n -0.0    0.65   0.0\n  0.57   0.11   0.14\n  0.69   0.0   -2.79\n  0.0    1.53  -1.14\n  1.69   0.0    0.29"
  },
  {
    "objectID": "posts/svar-identification/index.html#how-does-the-algorithm-work",
    "href": "posts/svar-identification/index.html#how-does-the-algorithm-work",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "How does the algorithm work?",
    "text": "How does the algorithm work?\nSo the algorithm works. But how does it work? It all comes down to the combination of Theorem 5 and Theorem 7. Theorem 7 tells us whether a SVAR is exactly identified. Theorem 5 tells us that if this is the case, then there exists a rotation matrix \\(P\\) such that \\((A_0, A_+)=(\\tilde A_0P, \\tilde A_+P)\\) where \\((A_0, A_+)\\) satisfies the restrictions, but \\((\\tilde A_0, \\tilde A_+)\\) does not. To find \\(P\\), note that \\(P\\) must satisfy \\[\nQ_jf(\\tilde A_0, \\tilde A_+)Pe_j = Q_jf(\\tilde A_0, \\tilde A_+)P_{\\cdot, j} = 0.\n\\] Thus, each \\(P_{\\cdot, j}\\) is an element in the nullspace of \\(Q_jf(\\tilde A_0, \\tilde A_+)\\) normalised to unit length. Focusing on \\(j=1\\), we can easily obtain \\(P_{\\cdot, 1}\\) by simply obtaining a vector within the nullspace of \\(Q_1f(\\tilde A_0, \\tilde A_+)\\) and normalising it to unit length. This is the first step of the algorithm.\nTo obtain the other columns, we need to be a bit more careful. We do not just need \\(P_{\\cdot, j}\\) to be in the nullspace of \\(Q_jf(\\tilde A_0, \\tilde A_+)\\), but we also need the resulting \\(P\\) to be a rotation matrix. We therefore need that \\(P_{\\cdot, j}'P_{\\cdot, k}=0\\) for all \\(j \\neq k\\). This can be achieved by also imposing that \\(P_{\\cdot, j}\\) is orthogonal to \\(P_{\\cdot, j-k}\\) for all \\(k=1, ... k-1\\). To get such a \\(P_{\\cdot, j}\\) we simply find a vector in the nullspace of \\[\n\\tilde Q_j =\n\\begin{bmatrix}\nQ_jf(\\tilde A_0, \\tilde A_+) \\\\\nP_{\\cdot, 1}' \\\\\n\\vdots \\\\\nP_{\\cdot, j-1}'\n\\end{bmatrix}.\n\\] Since the resulting \\(P_{\\cdot, j}\\) satisfies \\(\\tilde Q_j P_{\\cdot, j} = 0\\), we have \\[\n\\begin{split}\nQ_jf(\\tilde A_0, \\tilde A_+)P_{\\cdot, j} &= 0 \\\\\nP_{\\cdot, k}'P_{\\cdot, j} &= 0 \\;\\; \\forall 1 \\leq k \\leq j.\n\\end{split}\n\\] Thus, the resulting \\(P_{\\cdot, j}\\) satisfies all criteria and the \\(P\\) obtained in this way will be a rotation matrix such that \\((\\tilde A_0P, \\tilde A_+P) = (A_0, A_+)\\); the unique structural parameter point satisfying all restrictions."
  },
  {
    "objectID": "posts/svar-identification/index.html#partial-identification",
    "href": "posts/svar-identification/index.html#partial-identification",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "Partial Identification",
    "text": "Partial Identification\nSo far I have only discussed the results of Rubio-Ramirez, Waggoner, and Zha (2010) regarding exact identification. Exact identification implies that all shocks are identified. In actual empirical work, we rarely manage to find enough plausible restrictions to identify all shocks. Instead, we often only identify a subset of shocks. Rubio-Ramirez, Waggoner, and Zha (2010) also have results applying to partial identification. According to them, a structural parameter point \\((A_0, A_+) \\in R\\) is partially identified if and only if there does not exist another observationally equivalent parameter point \\((\\tilde A_0, \\tilde A_+) \\in R\\) such that \\(A_0e_j \\neq \\tilde A_0e_j\\) or \\(A_+e_j \\neq \\tilde A_+e_j\\) with \\(e_j\\) being the \\(j\\)th unit vector.\n\n\n\n\n\n\nImportant\n\n\n\nConsider an SVAR with admissible restrictions represented by \\(R\\). For \\((A_0, A_+) \\in R\\), if \\(M_i(f(A_0, A_+))\\) is of rank \\(n\\) for \\(1 \\leq i \\leq j\\), then the \\(j\\)th equation is globally identified at the paramter point \\((A_0, A_+)\\).\nNote: Order shocks such that \\(q_1 \\geq q_2 \\geq \\dots \\geq q_n\\).\n\n\nSince the \\(j\\)th equation being identified also implies that the \\(j\\)th shock is identified, Theorem 2 can be used to check whether an SVAR is partially identified. Theorem 2 relies on the \\(M_i(X)\\) matrix though, where \\(X\\) is a \\(k\\times n\\) matrix. This matrix is defined as \\[\n\\underbrace{M_i(X)}_{(k+i)\\times n} = \\begin{bmatrix}\nQ_iX \\\\\n\\begin{bmatrix}\nI_{j\\times j} & O_{j \\times (n-j)}\n\\end{bmatrix}\n\\end{bmatrix}.\n\\] A nice example of a partially identified SVAR, and how to apply Thoerem 2, is given in section 5.3 of Rubio-Ramirez, Waggoner, and Zha (2010).\nTheorem 2 can be used to check whether a specific equation in a SVAR is identified. However, it does not tell us how to find the partially identified SVAR from reduced form estimates. I could not find an explicit discussion of this in the paper. The algorithm above, however, can still be used to find a rotation matrix, that, upon post-multiplying it with the initial point, returns a structural parameter point that satisfies the restrictions. If the manual check implied that Theorem 2 holds for almost all structural parameter points, then the obtained parameter point uniquely identifies the \\(j\\)th equation and therefore the \\(j\\)th shock.\nRemark: Since the SVAR is only partially identified, the rotation matrix obtained is unlikely to be the only rotation matrix that can be used to obtain a structural parameter point that satisfies the restrictions. However, no matter which rotation matrix is returned, if the \\(j\\)th equation is identified at almost all structural parameter points, then all admissible rotation matrices will identify the exact same \\(j\\)th shock."
  },
  {
    "objectID": "posts/svar-identification/index.html#remaining-questions",
    "href": "posts/svar-identification/index.html#remaining-questions",
    "title": "Notes: Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference",
    "section": "Remaining Questions",
    "text": "Remaining Questions\nBelow are some of the questions that I still have and some initial thoughts on these questions if they exist.\n\nIs there some way to automate the check for partial identification?\nGiven multiple rotation matrices are feasible when the SVAR is only partially identified, is the above algorithm still efficient, or does there exist a more efficient algorithm? Do we maybe not need to find the entire rotation matrix?\n\n\nI do not think that we need to find the entire rotation matrix. In the partial identification case we are only interested in the \\(j\\)th equation. This can be obtained by finding the \\(j\\)th column of the rotation matrix. Thus, the algorithm could be stopped early in the partial identification case. However, for small SVARs, early stopping is unlikely to make a large time difference. It might be helpful in a Bayesian setting though, in which the rotation matrix must be found thousands of times.\n\n\nWhat is the best way to find the nullspace?\n\n\nRubio-Ramirez, Waggoner, and Zha (2010) propose to use the LU or QR decomposition, with the latter being used in the implementation above. However, Julia has a nullspace function which uses the SVD decomposition. Would it maybe be better to find the nullspace using the official function? I checked this and it seemed to work, but did not seem to make a large timing difference. I can imagine though that it makes a numerical accuracy difference."
  },
  {
    "objectID": "software/FredMDQD/index.html",
    "href": "software/FredMDQD/index.html",
    "title": "FredMDQD.jl",
    "section": "",
    "text": "Fred MD and Fred QD are two popular pre-compiled datasets for macroeconomic analysis and forecasting. FredMDQD makes working with these datasets easy. It has three key features:\nMore information can be found on the GitHub page."
  },
  {
    "objectID": "software/FredMDQD/index.html#downloading-the-most-recent-datasets",
    "href": "software/FredMDQD/index.html#downloading-the-most-recent-datasets",
    "title": "FredMDQD.jl",
    "section": "Downloading the most recent datasets",
    "text": "Downloading the most recent datasets\n\nusing FredMDQD\nfmd = FredMD()  # download most recent Fred MD\nfqd = FredQD()  # download most recent Fred QD"
  },
  {
    "objectID": "software/FredMDQD/index.html#downloading-a-specific-vintage",
    "href": "software/FredMDQD/index.html#downloading-a-specific-vintage",
    "title": "FredMDQD.jl",
    "section": "Downloading a specific vintage",
    "text": "Downloading a specific vintage\n\nusing FredMDQD\nusing Dates \n\nd = Date(\"2023/01\", dateformat\"yyyy/mm\")\nfmd = FredMD(d)  # download the January 2023 vintage of Fred MD\nfqd = FredQD(d)  # download the January 2024 vintage of Fred QD"
  },
  {
    "objectID": "software/FredMDQD/index.html#automatic-transformation-of-variables-according-to-the-recommended-codes",
    "href": "software/FredMDQD/index.html#automatic-transformation-of-variables-according-to-the-recommended-codes",
    "title": "FredMDQD.jl",
    "section": "Automatic transformation of variables according to the recommended codes",
    "text": "Automatic transformation of variables according to the recommended codes\n\nusing FredMDQD\n\nfmd = FredMD()\nfmd.transformed  # transformed data \nfmd.original  # original data\nfmd.tcodes  # transformation codes"
  },
  {
    "objectID": "software/FredMDQD/index.html#searching",
    "href": "software/FredMDQD/index.html#searching",
    "title": "FredMDQD.jl",
    "section": "Searching",
    "text": "Searching\n\nusing FredMDQD\n\n# Searching for federal funds rate\nsearch_appendix(:MD, \"fed\")\n\n# Searching for consumption\nsearch_appendix(:QD, \"consumption\")"
  },
  {
    "objectID": "software/BayesFluxJulia/index.html",
    "href": "software/BayesFluxJulia/index.html",
    "title": "BayesFlux.jl and BayesFluxR",
    "section": "",
    "text": "BayesFlux.jl extends the famous Flux.jl machine learning library in Julia to Bayesian Neural Networks (BNNs). It is not meant to be suitable for production, but instead is meant to allows easy research on BNNs. As such, I tried to keep all parts flexible and extensible. BayesFlux.jl is used in my master thesis investigating the potential usefulness of Bayesian LSTM networks for financial risk forecasting. It is also currently part of other Master thesis work.\nFor more information, please check out the GitHub repository. Any feedback is appreciated.\nTo make BayesFlux.jl accessible to users that do not know Julia, I also developed an interface to R, called BayesFluxR which can be found here. Both BayesFlux.jl and BayesFluxR are also available on the official Julia repository and CRAN respectively."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nFredMDQD.jl\n\n\nFredMDQD is a small Julia library for the popular Fred MD and Fred QD datasets\n\n\n\n\nTinyGibbs.jl\n\n\nTinyGibbs is a small Gibbs sampler that makes use of the AbstractMCMC interface. It therefore allows for efficient Gibbs sampling including parallel sampling of multiple chains. Additionally, TinyGibbs can collect samples in two ways: (1) as a dictionary of tensors where each tensor or (2) as a MCMCChains.Chains type. Therefore, all the funcionality of MCMCChains can be exploited with TinyGibbs.\n\n\n\n\nBayesFlux.jl and BayesFluxR\n\n\nBayesFlux.jl is a small but flexible Baysian Neural Network library. It is work that came out of my Master Thesis and Reseach Assitance. Both were supervised by Nalan Basturk.\n\n\n\n\nSnT_BigTime, SnT_VARS, and BigTime\n\n\nInteractive notebooks introducing VAR models and the BigTime R library.\n\n\n\n\n\n\nNo matching items"
  }
]