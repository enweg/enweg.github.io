[
  {
    "objectID": "posts/var-likelihood/index.html",
    "href": "posts/var-likelihood/index.html",
    "title": "Bayesian VARs - The Likelihood Function",
    "section": "",
    "text": "\\[\n\\newcommand{\\bm}{\\mathbf}\n\\newcommand{\\bA}{\\bm A}\n\\newcommand{\\ba}{\\bm a}\n\\newcommand{\\bB}{\\bm B}\n\\newcommand{\\bb}{\\bm b}\n\\newcommand{\\bC}{\\bm C}\n\\newcommand{\\bc}{\\bm c}\n\\newcommand{\\bD}{\\bm D}\n\\newcommand{\\bd}{\\bm d}\n\\newcommand{\\bE}{\\bm E}\n\\newcommand{\\be}{\\bm e}\n\\newcommand{\\bF}{\\bm F}\n\\newcommand{\\bof}{\\bm f}\n\\newcommand{\\bG}{\\bm G}\n\\newcommand{\\bg}{\\bm g}\n\\newcommand{\\bH}{\\bm H}\n\\newcommand{\\bh}{\\bm h}\n\\newcommand{\\bI}{\\bm I}\n\\newcommand{\\bi}{\\bm i}\n\\newcommand{\\bJ}{\\bm J}\n\\newcommand{\\bj}{\\bm j}\n\\newcommand{\\bK}{\\bm K}\n\\newcommand{\\bk}{\\bm k}\n\\newcommand{\\bL}{\\bm L}\n\\newcommand{\\bl}{\\bm l}\n\\newcommand{\\bM}{\\bm M}\n\\newcommand{\\bom}{\\bm m}\n\\newcommand{\\bN}{\\bm N}\n\\newcommand{\\bn}{\\bm n}\n\\newcommand{\\bO}{\\bm O}\n\\newcommand{\\bo}{\\bm o}\n\\newcommand{\\bP}{\\bm P}\n\\newcommand{\\bp}{\\bm p}\n\\newcommand{\\bQ}{\\bm Q}\n\\newcommand{\\bq}{\\bm q}\n\\newcommand{\\bR}{\\bm R}\n\\newcommand{\\br}{\\bm r}\n\\newcommand{\\bS}{\\bm S}\n\\newcommand{\\bs}{\\bm s}\n\\newcommand{\\bT}{\\bm T}\n\\newcommand{\\bt}{\\bm t}\n\\newcommand{\\bU}{\\bm U}\n\\newcommand{\\bu}{\\bm u}\n\\newcommand{\\bV}{\\bm V}\n\\newcommand{\\bv}{\\bm v}\n\\newcommand{\\bW}{\\bm W}\n\\newcommand{\\bw}{\\bm w}\n\\newcommand{\\bX}{\\bm X}\n\\newcommand{\\bx}{\\bm x}\n\\newcommand{\\bY}{\\bm Y}\n\\newcommand{\\by}{\\bm y}\n\\newcommand{\\bZ}{\\bm Z}\n\\newcommand{\\bz}{\\bm z}\n% bold Greek lowercase letters for vectors\n\\newcommand{\\balpha}{\\bm \\alpha}\n\\newcommand{\\bbeta}{\\bm \\beta}\n\\newcommand{\\bgamma}{\\bm \\gamma}\n\\newcommand{\\bdelta}{\\bm \\delta}\n\\newcommand{\\bepsilon}{\\bm \\epsilon}\n\\newcommand{\\bvarepsilon}{\\bm \\varepsilon}\n\\newcommand{\\bzeta}{\\bm \\zeta}\n\\newcommand{\\boeta}{\\bm \\eta}\n\\newcommand{\\btheta}{\\bm \\theta}\n\\newcommand{\\biota}{\\bm \\iota}\n\\newcommand{\\bkappa}{\\bm \\kappa}\n\\newcommand{\\blambda}{\\bm \\lambda}\n\\newcommand{\\bmu}{\\bm \\mu}\n\\newcommand{\\bnu}{\\bm \\nu}\n\\newcommand{\\bxi}{\\bm \\xi}\n\\newcommand{\\bpi}{\\bm \\pi}\n\\newcommand{\\brho}{\\bm \\rho}\n\\newcommand{\\bsigma}{\\bm \\sigma}\n\\newcommand{\\btau}{\\bm \\tau}\n\\newcommand{\\bupsilon}{\\bm \\upsilon}\n\\newcommand{\\bphi}{\\bm \\phi}\n\\newcommand{\\bchi}{\\bm \\chi}\n\\newcommand{\\bpsi}{\\bm \\psi}\n\\newcommand{\\bomega}{\\bm \\omega}\n\\]\nA standard Baysian Vector AutoRegression (VAR) takes the form\n\\[\n\\mathbf{y}_t = C\\bar{\\mathbf{y}}_t + \\sum_{i=1}^p A_i \\mathbf{y}_{t-i} + \\mathbf{\\varepsilon}_t\n\\]\nwhere \\(\\mathbf{y}_t\\) is a \\(m \\times 1\\) vector of endogenous time series, \\(\\bar{\\mathbf{y}}_t\\) is a \\(m_c \\times 1\\) vector of constants and other deterministic trends, and \\(A_i\\) for \\(i=1, ..., p\\) and \\(C\\) are coefficient matrices. In total, there are \\(k\\equiv mp + m_c\\) coefficients in each equation, resulting in a total of \\(mk\\) coefficients in the model.\nWithout further specifying the distribution for the error terms, \\(\\mathbf{\\varepsilon}_t\\), no likelihood can be derived. The most common choice is to assume that the errors follow a multivariate Normal distribution with mean zero and covariance matrix \\(\\Sigma\\). That is, most commonly we assume that \\(\\mathbf{\\varepsilon}_t \\sim N(\\mathbf{0}, \\Sigma)\\). This then implies that\n\\[\n\\mathbf{y}_t | C, A_1, ..., A_p, \\Sigma, \\mathbf{y}_{t-1}, ..., \\mathbf{y}_{t-p} \\sim N(C\\bar{\\mathbf{y}}_t + \\sum_{i=1}^p A_i \\mathbf{y}_{t-i}, \\Sigma)\n\\]\nwhich is then used to derive the for of the likelihood that is classically used. For the Bayesian setting, it turns out, that there is a more convenient form of the likelihood. While both are functionally equivalent, the way Baysians write down the likelihood allows them to easily calculate the posterior given a prior on all parameters.\nTo derive the more convenient form, define \\(A = [c \\quad A_1 \\quad ... \\quad A_p]'\\) and \\(\\mathbf{x}_t = [\\bar{\\mathbf{y}}_t', \\mathbf{y}_{t-1}', ..., \\mathbf{y}_{t-p}']\\) where the former is of dimenions \\(k \\times m\\) and the latter is a row vector of dimensions \\(1\\times k\\). Using these definitions, we can write the VAR as\n\\[\n\\mathbf{y}_t = A'\\mathbf{x}_t' + \\mathbf{\\varepsilon}_t \\Rightarrow \\mathbf{y}_t' = \\mathbf{x}_t A + \\mathbf{\\varepsilon}_t'\n\\]\nThis form is convenient, because it allows us to define\n\\[\n\\begin{array}{ccc}\nY = \\begin{bmatrix}\\mathbf{y}_1' \\\\ \\vdots \\\\ \\mathbf{y}_T'\\end{bmatrix} &\nX = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\vdots \\\\ \\mathbf{x}_T\\end{bmatrix} &\nE = \\begin{bmatrix}\\mathbf{\\varepsilon}_1 \\\\ \\vdots \\\\ \\mathbf{\\varepsilon}_T\\end{bmatrix}\n\\end{array}\n\\]\nWe can then write the VAR in matrix notation as\n\\[\n\\DeclareMathOperator{\\vect}{vec}\n\\DeclareMathOperator{\\cov}{cov}\nY = XA + E \\Rightarrow \\mathbf{y} = (I_m \\otimes X)\\mathbf{\\alpha} + \\mathbf{\\varepsilon}\n\\]\nwhere \\(y = \\vect(Y)\\), \\(\\mathbf{\\alpha} = vec(A)\\) and \\(\\mathbf{\\varepsilon} = \\vect(E)\\). The vectorised form follows directly from the rules of the \\(\\vect\\) operator which can be found in e.g.¬†the matrix cookbook.\nWhat are the distributions of \\(E\\) and \\(\\mathbf{\\varepsilon}\\) though? I will pospone the discussion of the distribution of \\(E\\) to another post, since it is of Matrix Normal form and not many seem to know of the Matrix Normal distributions. Instead, I will focus on the distribution of \\(\\mathbf{\\varepsilon}\\) which is Multivariate Normal which is directly inhereted form the Multivarite Normal distribution of \\(\\mathbf{\\varepsilon}_t\\). To fully describe the distributions, we need its mean and covariance. The mean is easy; since every \\(\\varepsilon_{it}\\) has mean \\(\\mathbf{0}\\), we must have that \\(\\mathbf{\\varepsilon}\\) has mean \\(\\mathbf{0}\\). The covariance is a bit more tricky, and it helps to consider a small example case.\nLets suppose for a minute that \\(T=2\\) and \\(m=2\\). Then, \\(\\mathbf{\\varepsilon} = [\\varepsilon_{11}, \\varepsilon_{12}, \\varepsilon_{21}, \\varepsilon_{22}]'\\). Since errors are uncorrelated over time, we have in general that \\(\\cov(\\varepsilon_{it}, \\varepsilon_{jt'})=0\\) for all \\(i,j \\in \\{1, ...., m\\}\\) and all \\(t\\neq t'\\). We also know the covariance of any two errors at the same point in time: \\(\\cov(\\varepsilon_{it}, \\varepsilon_{jt})=\\sigma_{ij}=[\\Sigma]_{ij}\\) - the \\(ij\\) element of the covariance matrix \\(\\Sigma\\). Thus, in our small example, the covariance structure of \\(\\mathbf{\\varepsilon}\\) must take the form\n\\[\n\\begin{bmatrix}\n\\sigma_{11} & 0 & \\sigma_{12} & 0 \\\\\n0 & \\sigma_{11} & 0 & \\sigma_{12} \\\\\n\\sigma_{21} & 0 & \\sigma_{22} & 0 \\\\\n0 & \\sigma_{21} & 0 & \\sigma_{22}\n\\end{bmatrix} = \\Sigma \\otimes I_T\n\\]\nA careful look at the form above reveals that it is equal to \\(\\Sigma \\otimes I_T\\). This generally holds, implying that \\(\\mathbf{\\varepsilon} \\sim N(\\mathbf{0}, \\Sigma \\otimes I_T)\\).\n\n\n\n\n\n\nA small useful trick\n\n\n\n\n\nBefore we continue out derivation of the likelihood, it is useful to first derive a small algebraic trick. The general problem is, that we are given a quadratic form\n\\[\n(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta)\n\\]\nand we want to rewrite the above into a quadratic form in \\(\\bbeta\\) and \\(\\by\\). We thus first write\n\\[\n\\begin{split}\n&(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta) \\\\\n=& (\\by - X\\hat\\bbeta + X\\hat\\bbeta X\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta + X\\hat\\bbeta X\\bbeta) \\\\  \n=& (\\by - X\\hat\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta) + (\\bbeta - \\hat\\bbeta)X'\\Sigma^{-1}X(\\bbeta - \\hat\\bbeta) + 2(\\by - X\\hat\\bbeta)'\\Sigma^{-1}(X\\hat\\bbeta - X\\bbeta)\n\\end{split}\n\\]\nWe would like to get rid of the last term by choosing a smart \\(\\hat\\bbeta\\neq \\bbeta\\). For the last term to be zero, it is sufficient to find a \\(\\hat\\bbeta\\) that sets \\(\\by - X\\hat\\bbeta = 0\\).\n\\[\n\\begin{split}\n&\\by - X\\hat\\bbeta = 0 \\\\\n\\Leftrightarrow & \\by = X\\hat\\bbeta \\\\\n\\Leftrightarrow & X'\\by = X'X\\hat\\bbeta \\\\\n\\Rightarrow & \\hat\\bbeta = (X'X)^{-1}X'\\by\n\\end{split}\n\\]\nThus, by choosing \\(\\hat\\bbeta\\) to be the standard OLS estimate, we can rewrite the quadratic involving both \\(\\by\\) and \\(\\bbeta\\) into two quadratic forms, each involving only one of \\(\\by\\) and \\(\\bbeta\\). That is, for \\(\\hat\\bbeta = (X'X)^{-1}X'\\by\\) we have\n\\[\n(\\by - X\\bbeta)'\\Sigma^{-1}(\\by - X\\bbeta) = (\\by - X\\hat\\bbeta)'\\Sigma^{-1}(\\by - X\\hat\\bbeta) + (\\bbeta - \\hat\\bbeta)'X'\\Sigma^{-1}X(\\bbeta - \\hat\\bbeta)\n\\]\n\n\n\nSo we know that \\(\\by = (I_m \\otimes X)\\balpha + \\bvarepsilon\\) with \\(\\bvarepsilon \\sim N(\\bm 0, \\Sigma \\otimes I_T)\\). Thus, we also know \\(\\by \\sim N((I_m \\otimes X)\\balpha, \\Sigma \\otimes I_T)\\). Looking up the density of the multivariate Normal on, for example, wikipedia reveals that the likelihood can be proportially by written as\n\\[\n\\mathcal{L}(\\balpha, \\Sigma) \\propto |\\Sigma \\otimes I_T|^{-1/2}\\exp\\{-\\frac{1}{2}(\\by - (I_m \\times X)\\balpha)'(\\Sigma \\otimes I_T)^{-1}(\\by - (I_m \\otimes X)\\balpha)\\}\n\\]\nUsing the small trick above, we can rewrite this as\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma \\otimes I_T|^{-1/2}\\exp\\{-\\frac{1}{2}(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma \\times I_T)^{-1}(\\by - (I_m \\otimes X)\\hat\\balpha)\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(I_m\\otimes X)'(\\Sigma \\otimes I_T)(I_m \\otimes X)(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nwhere \\(\\hat\\balpha = (I_m \\otimes (X'X)^{-1}X')\\by\\). This can be massively simplified by making the following observations (see for example the matrix cookbook)\n\n\\(|A \\otimes B| = |A|^{rank(B)}|B|^{rank(A)}\\) and thus \\(|\\Sigma \\otimes I_T| = |\\Sigma|^T\\)\n\\((A \\otimes B)' = A' \\otimes B'\\) and thus \\((I_m \\otimes X)' = I_m \\otimes X'\\)\n\\((A \\otimes B)^{-1} = A^{-1}\\otimes B^{-1}\\) if the inverses exist. Thus \\((\\Sigma \\otimes I_T)^{-1} = \\Sigma^{-1}\\otimes I_T\\)\nIf all matrices are comformable, then \\((A\\otimes B)(C\\otimes D) = AD\\otimes BC\\) and thus\n\n\\[\n(I_m\\otimes X)'(\\Sigma\\otimes I_T)^{-1}(I_m \\otimes X) = (\\Sigma \\otimes (X'X)^{-1})^{-1}\n\\]\nUsing all of this in the likelihood, we can write\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-T/2}\\exp\\{\\frac{1}{2}(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nBefore we can come to the conclusion of this derivation, we need to rewrite one more part. For this, consider the first exponent term and write\n\\[\n\\begin{split}\n&(\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m\\otimes X)\\hat\\balpha)\\\\\n= &[(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)]'[(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha)]\n\\end{split}\n\\]\nCarefully looking at some of the kronecker product and \\(\\vect\\) operator properties, we can then recognise that\n\\[\n(\\Sigma^{-1/2}\\otimes I_T)(\\by - (I_m \\otimes X)\\hat\\balpha) = \\vect((Y - X\\hat A)\\Sigma^{-1/2})\n\\]\nwhere \\(\\hat\\balpha = \\vect(\\hat A)\\). Thus, the term in the first exponent can be written as\n\\[\n\\begin{split}\n& (\\by - (I_m \\otimes X)\\hat\\balpha)'(\\Sigma^{-1}\\otimes I_T)(\\by - (I_m\\otimes X)\\hat\\balpha) \\\\\n= &\\vect((Y - X\\hat A)\\Sigma^{-1/2})'\\vect((Y - X\\hat A)\\Sigma^{-1/2})\n\\end{split}\n\\]\nBy the properties of the trace, this equals\n\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\begin{split}\n\\vect((Y - X\\hat A)\\Sigma^{-1/2})'\\vect((Y - X\\hat A)\\Sigma^{-1/2}) &= \\tr((\\Sigma^{-1/2})'(Y-X\\hat A)'(Y-X\\hat A)\\Sigma^{-1/2}) \\\\\n&= \\tr(\\Sigma^{-1/2}(\\Sigma^{-1/2})'(Y - X\\hat A)'(Y-X \\hat A)) \\\\\n&= \\tr(\\Sigma^{-1}(Y - X\\hat A)'(Y - X\\hat A)) \\\\\n&= \\tr(\\underbrace{(Y-X\\hat A)'(Y - X\\hat A)}_{S}\\Sigma^{-1})\n\\end{split}\n\\]\nWe then finally arrive at the result that we were heading for\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-T/2}\\exp\\{\\frac{1}{2}\\tr(S\\Sigma^{-1})\\} \\\\\n&\\times \\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\}\n\\end{split}\n\\]\nWhy is this result so useful? Carefully looking at the expression above and comparing the forms to some known distributions, you might notice that the second exponential looks very much like a multivariate Normal distribution, while the first part looks very much like an Inverse-Wishart distribution. To actually get there, we still need to multiply and devide by \\(|\\Sigma \\otimes (X'X)^{-1}|^{-1/2}\\) which by the rules of the kronecker product is the same as \\(|\\Sigma|^{-k/2}|(X'X)^{-1}|^{-m/2}\\). Thus, after ignoring \\(|(X'X)^{-1}|^{-m/2}\\) because it does not depend on \\(\\balpha\\) or \\(\\Sigma\\), we can finally write the likelihood as\n\\[\n\\begin{split}\n\\mathcal{L}(\\balpha, \\Sigma) &\\propto |\\Sigma|^{-(T-k)/2}\\exp\\{\\frac{1}{2}\\tr(S\\Sigma^{-1})\\} \\\\\n&\\times |\\Sigma \\otimes (X'X)^{-1}|^{-1/2}\\exp\\{-\\frac{1}{2}(\\balpha - \\hat\\balpha)'(\\Sigma \\otimes (X'X)^{-1})^{-1}(\\balpha - \\hat\\balpha)\\} \\\\\n&=p(\\Sigma)p(\\balpha|\\Sigma)\n\\end{split}\n\\]\nIn the last like above I have already indicated that this is proportional to the product of a marginal distribution for \\(\\Sigma\\) and a conditional distribution for \\(\\balpha\\). From the functional forms we find that\n\\[\n\\begin{split}\n\\Sigma &\\sim IW(S, T-k-m-1) \\\\\n\\balpha | \\Sigma &\\sim N(\\hat\\balpha, \\Sigma \\otimes (X'X)^{-1})\n\\end{split}\n\\]\nSo again, why is this useful? Because it turns out that the likelihood is proportional to these two distributions. Why is that in turn useful? Because we know how to design smart priors for each part. That is, we know how to design a smart prior for the Inverse-Wishart part and we know how to design a smart prior for the conditional Normal part. With smart, I mean, among others, that we know conjugate priors for both parts. But even for non-conjugate priors, the form above is often more convenient to use than the classical form of the likelihood. I will show these benefits in future posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi üëãüèª",
    "section": "",
    "text": "I‚Äôm Enrico. I am a PhD candiate at the School of Business and Economics Maastricht. I am interested in causal and Bayesian macro-econometrics."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Bayesian VARs - The Likelihood Function\n\n\n\n\n\n(Bayesian) VARs form the backbone of modern macroeconometrics, but deriving the likelihood of a VAR in a form that is useful for Baysian calculations is not necessarily the most straighforward task. In this blog post I show how the VAR likelihood can be written in a form that is useful for Bayesian calculations.\n\n\n\n\n\n\nMay 7, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software/TinyGibbs/index.html",
    "href": "software/TinyGibbs/index.html",
    "title": "TinyGibbs.jl",
    "section": "",
    "text": "While learning more about VAR models, I noticed that many of the BayesianEstimation methods rely on Gibbs sampling. Although Julia has many Bayesian libraries, I could not find a Gibbs sampling library that I quite liked. What I wanted was a library that allows one to write a sampler just like one would dicsuss on in a paper. That is, I wanted a sampler that abstract away all the actual computational work and lets me focus on the actual conditional distributions. What I wanted was a sampler that lets me translate a statement like\n\nSample \\(\\alpha\\) from \\(p(\\alpha | y, x, \\Sigma)\\)\nSample \\(\\Sigma\\) from \\(p(\\Sigma | y, x, \\alpha)\\)\n\ninto a valid sampler.\nTinyGibbs is my answer to this need. TinyGibbs introduced a single macro, @tiny_gibbs which transforms a statement that is as close as possible to actual pseudo code in papers into a valid sampler. Additionally, by exploiting the functionality provided in AbstractMCMC and MCMCChains, TinyGibbs is able to sample in parallel and to diagnose the resulting MCMC chains.\nFor more information and to try it out, check out the GitHub repo. Any feedback is greatly appreciated."
  },
  {
    "objectID": "software/BayesFluxJulia/index.html",
    "href": "software/BayesFluxJulia/index.html",
    "title": "BayesFlux.jl and BayesFluxR",
    "section": "",
    "text": "BayesFlux.jl extends the famous Flux.jl machine learning library in Julia to Bayesian Neural Networks (BNNs). It is not meant to be suitable for production, but instead is meant to allows easy research on BNNs. As such, I tried to keep all parts flexible and extensible. BayesFlux.jl is used in my master thesis investigating the potential usefulness of Bayesian LSTM networks for financial risk forecasting. It is also currently part of other Master thesis work.\nFor more information, please check out the GitHub repository. Any feedback is appreciated.\nTo make BayesFlux.jl accessible to users that do not know Julia, I also developed an interface to R, called BayesFluxR which can be found here. Both BayesFlux.jl and BayesFluxR are also available on the official Julia repository and CRAN respectively."
  },
  {
    "objectID": "software/SnT/index.html",
    "href": "software/SnT/index.html",
    "title": "SnT_BigTime, SnT_VARS, and BigTime",
    "section": "",
    "text": "As part of my student assistantship to Ines Wilms, I got the chance to develop a set of interactive notebooks introducing VAR models and the BigTime library to which I got to contribute minor parts. These notebooks can be checked out on their respective GitHub repositories.\n\nIntroduction to VARs\nIntroduction to BigTime\nBigTime library"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nTinyGibbs.jl\n\n\nTinyGibbs is a small Gibbs sampler that makes use of the AbstractMCMC interface. It therefore allows for efficient Gibbs sampling including parallel sampling of multiple chains. Additionally, TinyGibbs can collect samples in two ways: (1) as a dictionary of tensors where each tensor or (2) as a MCMCChains.Chains type. Therefore, all the funcionality of MCMCChains can be exploited with TinyGibbs.\n\n\n\n\nBayesFlux.jl and BayesFluxR\n\n\nBayesFlux.jl is a small but flexible Baysian Neural Network library. It is work that came out of my Master Thesis and Reseach Assitance. Both were supervised by Nalan Basturk.\n\n\n\n\nSnT_BigTime, SnT_VARS, and BigTime\n\n\nInteractive notebooks introducing VAR models and the BigTime R library.\n\n\n\n\n\n\nNo matching items"
  }
]